{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. mlpmodel.ipynb(객체형)"
      ],
      "metadata": {
        "id": "d7d1lExEOBBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(1234)\n",
        "def randomize(): np.random.seed(time.time())\n",
        "class Model(object):\n",
        "    def __init__(self, name, dataset):\n",
        "        self.name = name\n",
        "        self.dataset = dataset\n",
        "        self.is_training = False\n",
        "        if not hasattr(self, 'rand_std'): self.rand_std = 0.030\n",
        "\n",
        "    def __str__(self):\n",
        "        return '{}/{}'.format(self.name, self.dataset)\n",
        "\n",
        "    def exec_all(self, epoch_count=10, batch_size=10, learning_rate=0.01,\n",
        "                 report=0, show_cnt=3):\n",
        "        self.train(epoch_count, batch_size, learning_rate, report)\n",
        "        self.test()\n",
        "        if show_cnt > 0: self.visualize(show_cnt)\n",
        "class MlpModel(Model):\n",
        "    def __init__(self, name, dataset, hconfigs):\n",
        "        super(MlpModel, self).__init__(name, dataset)\n",
        "        self.init_parameters(hconfigs)\n",
        "def mlp_init_parameters(self, hconfigs):\n",
        "    self.hconfigs = hconfigs\n",
        "    self.pm_hiddens = []\n",
        "\n",
        "    prev_shape = self.dataset.input_shape\n",
        "\n",
        "    for hconfig in hconfigs:\n",
        "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig)\n",
        "        self.pm_hiddens.append(pm_hidden)\n",
        "\n",
        "    output_cnt = int(np.prod(self.dataset.output_shape))\n",
        "    self.pm_output, _ = self.alloc_layer_param(prev_shape, output_cnt)\n",
        "\n",
        "def mlp_alloc_layer_param(self, input_shape, hconfig):\n",
        "    input_cnt = np.prod(input_shape)\n",
        "    output_cnt = hconfig\n",
        "\n",
        "    weight, bias = self.alloc_param_pair([input_cnt, output_cnt])\n",
        "\n",
        "    return {'w':weight, 'b':bias}, output_cnt\n",
        "\n",
        "def mlp_alloc_param_pair(self, shape):\n",
        "    weight = np.random.normal(0, self.rand_std, shape)\n",
        "    bias = np.zeros([shape[-1]])\n",
        "    return weight, bias\n",
        "\n",
        "MlpModel.init_parameters = mlp_init_parameters\n",
        "MlpModel.alloc_layer_param = mlp_alloc_layer_param\n",
        "MlpModel.alloc_param_pair = mlp_alloc_param_pair\n",
        "def mlp_model_train(self, epoch_count=10, batch_size=10, \\\n",
        "                    learning_rate=0.001, report=0):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    batch_count = int(self.dataset.train_count / batch_size)\n",
        "    time1 = time2 = int(time.time())\n",
        "    if report != 0:\n",
        "        print('Model {} train started:'.format(self.name))\n",
        "\n",
        "    for epoch in range(epoch_count):\n",
        "        costs = []\n",
        "        accs = []\n",
        "        self.dataset.shuffle_train_data(batch_size*batch_count)\n",
        "        for n in range(batch_count):\n",
        "            trX, trY = self.dataset.get_train_data(batch_size, n)\n",
        "            cost, acc = self.train_step(trX, trY)\n",
        "            costs.append(cost)\n",
        "            accs.append(acc)\n",
        "\n",
        "        if report > 0 and (epoch+1) % report == 0:\n",
        "            vaX, vaY = self.dataset.get_validate_data(100)\n",
        "            acc = self.eval_accuracy(vaX, vaY)\n",
        "            time3 = int(time.time())\n",
        "            tm1, tm2 = time3-time2, time3-time1\n",
        "            self.dataset.train_prt_result(epoch+1, costs, accs, acc, tm1, tm2)\n",
        "            time2 = time3\n",
        "\n",
        "    tm_total = int(time.time()) - time1\n",
        "    print('Model {} train ended in {} secs:'.format(self.name, tm_total))\n",
        "\n",
        "MlpModel.train = mlp_model_train\n",
        "def mlp_model_test(self):\n",
        "    teX, teY = self.dataset.get_test_data()\n",
        "    time1 = int(time.time())\n",
        "    acc = self.eval_accuracy(teX, teY)\n",
        "    time2 = int(time.time())\n",
        "    self.dataset.test_prt_result(self.name, acc, time2-time1)\n",
        "\n",
        "MlpModel.test = mlp_model_test\n",
        "def mlp_model_visualize(self, num):\n",
        "    print('Model {} Visualization'.format(self.name))\n",
        "    deX, deY = self.dataset.get_visualize_data(num)\n",
        "    est = self.get_estimate(deX)\n",
        "    self.dataset.visualize(deX, est, deY)\n",
        "\n",
        "MlpModel.visualize = mlp_model_visualize\n",
        "def mlp_train_step(self, x, y):\n",
        "    self.is_training = True\n",
        "\n",
        "    output, aux_nn = self.forward_neuralnet(x)\n",
        "    loss, aux_pp = self.forward_postproc(output, y)\n",
        "    accuracy = self.eval_accuracy(x, y, output)\n",
        "\n",
        "    G_loss = 1.0\n",
        "    G_output = self.backprop_postproc(G_loss, aux_pp)\n",
        "    self.backprop_neuralnet(G_output, aux_nn)\n",
        "\n",
        "    self.is_training = False\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "MlpModel.train_step = mlp_train_step\n",
        "def mlp_forward_neuralnet(self, x):\n",
        "    hidden = x\n",
        "    aux_layers = []\n",
        "\n",
        "    for n, hconfig in enumerate(self.hconfigs):\n",
        "        hidden, aux = self.forward_layer(hidden, hconfig, self.pm_hiddens[n])\n",
        "        aux_layers.append(aux)\n",
        "\n",
        "    output, aux_out = self.forward_layer(hidden, None, self.pm_output)\n",
        "\n",
        "    return output, [aux_out, aux_layers]\n",
        "\n",
        "def mlp_backprop_neuralnet(self, G_output, aux):\n",
        "    aux_out, aux_layers = aux\n",
        "\n",
        "    G_hidden = self.backprop_layer(G_output, None, self.pm_output, aux_out)\n",
        "\n",
        "    for n in reversed(range(len(self.hconfigs))):\n",
        "        hconfig, pm, aux = self.hconfigs[n], self.pm_hiddens[n], aux_layers[n]\n",
        "        G_hidden = self.backprop_layer(G_hidden, hconfig, pm, aux)\n",
        "\n",
        "    return G_hidden\n",
        "\n",
        "MlpModel.forward_neuralnet = mlp_forward_neuralnet\n",
        "MlpModel.backprop_neuralnet = mlp_backprop_neuralnet\n",
        "def mlp_forward_layer(self, x, hconfig, pm):\n",
        "    y = np.matmul(x, pm['w']) + pm['b']\n",
        "    if hconfig is not None: y = relu(y)\n",
        "    return y, [x,y]\n",
        "\n",
        "def mlp_backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "    x, y = aux\n",
        "\n",
        "    if hconfig is not None: G_y = relu_derv(y) * G_y\n",
        "\n",
        "    g_y_weight = x.transpose()\n",
        "    g_y_input = pm['w'].transpose()\n",
        "\n",
        "    G_weight = np.matmul(g_y_weight, G_y)\n",
        "    G_bias = np.sum(G_y, axis=0)\n",
        "    G_input = np.matmul(G_y, g_y_input)\n",
        "\n",
        "    pm['w'] -= self.learning_rate * G_weight\n",
        "    pm['b'] -= self.learning_rate * G_bias\n",
        "\n",
        "    return G_input\n",
        "\n",
        "MlpModel.forward_layer = mlp_forward_layer\n",
        "MlpModel.backprop_layer = mlp_backprop_layer\n",
        "def mlp_forward_postproc(self, output, y):\n",
        "    loss, aux_loss = self.dataset.forward_postproc(output, y)\n",
        "    extra, aux_extra = self.forward_extra_cost(y)\n",
        "    return loss + extra, [aux_loss, aux_extra]\n",
        "\n",
        "def mlp_forward_extra_cost(self, y):\n",
        "    return 0, None\n",
        "\n",
        "MlpModel.forward_postproc = mlp_forward_postproc\n",
        "MlpModel.forward_extra_cost = mlp_forward_extra_cost\n",
        "def mlp_backprop_postproc(self, G_loss, aux):\n",
        "    aux_loss, aux_extra = aux\n",
        "    self.backprop_extra_cost(G_loss, aux_extra)\n",
        "    G_output = self.dataset.backprop_postproc(G_loss, aux_loss)\n",
        "    return G_output\n",
        "\n",
        "def mlp_backprop_extra_cost(self, G_loss, aux):\n",
        "    pass\n",
        "\n",
        "MlpModel.backprop_postproc = mlp_backprop_postproc\n",
        "MlpModel.backprop_extra_cost = mlp_backprop_extra_cost\n",
        "def mlp_eval_accuracy(self, x, y, output=None):\n",
        "    if output is None:\n",
        "        output, _ = self.forward_neuralnet(x)\n",
        "    accuracy = self.dataset.eval_accuracy(x, y, output)\n",
        "    return accuracy\n",
        "\n",
        "MlpModel.eval_accuracy = mlp_eval_accuracy\n",
        "def mlp_get_estimate(self, x):\n",
        "    output, _ = self.forward_neuralnet(x)\n",
        "    estimate = self.dataset.get_estimate(output)\n",
        "    return estimate\n",
        "\n",
        "MlpModel.get_estimate = mlp_get_estimate"
      ],
      "metadata": {
        "id": "ZU-WlyqGO12E"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "import copy\n",
        "import wave\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "def randomize():\n",
        "    np.random.seed(time.time())\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, name, dataset):\n",
        "        self.name = name\n",
        "        self.dataset = dataset\n",
        "        self.is_training = False\n",
        "        if not hasattr(self, 'rand_std'):\n",
        "            self.rand_std = 0.030\n",
        "\n",
        "    def __str__(self):\n",
        "        return '{}/{}'.format(self.name, self.dataset)\n",
        "\n",
        "    def exec_all(self, epoch_count=10, batch_size=10, learning_rate=0.01, report=0, show_cnt=3):\n",
        "        self.train(epoch_count, batch_size, learning_rate, report)\n",
        "        self.test()\n",
        "        if show_cnt > 0:\n",
        "            self.visualize(show_cnt)\n",
        "\n",
        "class MlpModel(Model):\n",
        "    def __init__(self, name, dataset, hconfigs, dropout_p=None):\n",
        "        super(MlpModel, self).__init__(name, dataset)\n",
        "        self.dropout_p = dropout_p\n",
        "        self.init_parameters(hconfigs)\n",
        "\n",
        "    def init_parameters(self, hconfigs):\n",
        "        self.hconfigs = hconfigs\n",
        "        self.pm_hiddens = []\n",
        "        prev_shape = self.dataset.input_shape\n",
        "        for hconfig in hconfigs:\n",
        "            pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig)\n",
        "            self.pm_hiddens.append(pm_hidden)\n",
        "        output_cnt = int(np.prod(self.dataset.output_shape))\n",
        "        self.pm_output, _ = self.alloc_layer_param(prev_shape, output_cnt)\n",
        "\n",
        "    def alloc_layer_param(self, input_shape, hconfig):\n",
        "        input_cnt = np.prod(input_shape)\n",
        "        output_cnt = hconfig\n",
        "        weight, bias = self.alloc_param_pair([input_cnt, output_cnt])\n",
        "        return {'w': weight, 'b': bias}, output_cnt\n",
        "\n",
        "    def alloc_param_pair(self, shape):\n",
        "        weight = np.random.normal(0, self.rand_std, shape)\n",
        "        bias = np.zeros([shape[-1]])\n",
        "        return weight, bias\n",
        "\n",
        "    def train(self, epoch_count=10, batch_size=10, learning_rate=0.001, report=0):\n",
        "        self.learning_rate = learning_rate\n",
        "        batch_count = int(self.dataset.train_count / batch_size)\n",
        "        time1 = time2 = int(time.time())\n",
        "        if report != 0:\n",
        "            print('Model {} train started:'.format(self.name))\n",
        "        for epoch in range(epoch_count):\n",
        "            costs = []\n",
        "            accs = []\n",
        "            self.dataset.shuffle_train_data(batch_size * batch_count)\n",
        "            for n in range(batch_count):\n",
        "                trX, trY = self.dataset.get_train_data(batch_size, n)\n",
        "                cost, acc = self.train_step(trX, trY)\n",
        "                costs.append(cost)\n",
        "                accs.append(acc)\n",
        "            if report > 0 and (epoch + 1) % report == 0:\n",
        "                vaX, vaY = self.dataset.get_validate_data(100)\n",
        "                acc = self.eval_accuracy(vaX, vaY)\n",
        "                time3 = int(time.time())\n",
        "                tm1, tm2 = time3 - time2, time3 - time1\n",
        "                self.dataset.train_prt_result(epoch + 1, costs, accs, acc, tm1, tm2)\n",
        "                time2 = time3\n",
        "        tm_total = int(time.time()) - time1\n",
        "        print('Model {} train ended in {} secs:'.format(self.name, tm_total))\n",
        "\n",
        "    def test(self):\n",
        "        teX, teY = self.dataset.get_test_data()\n",
        "        time1 = int(time.time())\n",
        "        acc = self.eval_accuracy(teX, teY)\n",
        "        time2 = int(time.time())\n",
        "        self.dataset.test_prt_result(self.name, acc, time2 - time1)\n",
        "\n",
        "    def visualize(self, num):\n",
        "        print('Model {} Visualization'.format(self.name))\n",
        "        deX, deY = self.dataset.get_visualize_data(num)\n",
        "        est = self.get_estimate(deX)\n",
        "        self.dataset.visualize(deX, est, deY)\n",
        "\n",
        "    def train_step(self, x, y):\n",
        "        self.is_training = True\n",
        "        output, aux_nn = self.forward_neuralnet(x)\n",
        "        loss, aux_pp = self.forward_postproc(output, y)\n",
        "        accuracy = self.eval_accuracy(x, y, output)\n",
        "        G_loss = 1.0\n",
        "        G_output = self.backprop_postproc(G_loss, aux_pp)\n",
        "        self.backprop_neuralnet(G_output, aux_nn)\n",
        "        self.is_training = False\n",
        "        return loss, accuracy\n",
        "\n",
        "    def forward_neuralnet(self, x):\n",
        "        hidden = x\n",
        "        aux_layers = []\n",
        "        for n, hconfig in enumerate(self.hconfigs):\n",
        "            hidden, aux = self.forward_layer(hidden, hconfig, self.pm_hiddens[n])\n",
        "            aux_layers.append(aux)\n",
        "        output, aux_out = self.forward_layer(hidden, None, self.pm_output)\n",
        "        return output, [aux_out, aux_layers]\n",
        "\n",
        "    def backprop_neuralnet(self, G_output, aux):\n",
        "        aux_out, aux_layers = aux\n",
        "        G_hidden = self.backprop_layer(G_output, None, self.pm_output, aux_out)\n",
        "        for n in reversed(range(len(self.hconfigs))):\n",
        "            hconfig, pm, aux = self.hconfigs[n], self.pm_hiddens[n], aux_layers[n]\n",
        "            G_hidden = self.backprop_layer(G_hidden, hconfig, pm, aux)\n",
        "        return G_hidden\n",
        "\n",
        "    def forward_layer(self, x, hconfig, pm):\n",
        "        y = np.matmul(x, pm['w']) + pm['b']\n",
        "        if hconfig is not None:\n",
        "            y = relu(y)\n",
        "            if self.dropout_p is not None and self.is_training:\n",
        "                dropout_mask = np.random.binomial(1, self.dropout_p, size=y.shape) / self.dropout_p\n",
        "                y *= dropout_mask\n",
        "                return y, [x, y, dropout_mask]\n",
        "            return y, [x, y]\n",
        "        return y, [x, y]\n",
        "\n",
        "    def backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "        if self.dropout_p is not None and self.is_training and hconfig is not None:\n",
        "            x, y, dropout_mask = aux\n",
        "            G_y *= dropout_mask\n",
        "        else:\n",
        "            x, y = aux\n",
        "        if hconfig is not None:\n",
        "            G_y = relu_derv(y) * G_y\n",
        "        g_y_weight = x.transpose()\n",
        "        g_y_input = pm['w'].transpose()\n",
        "        G_weight = np.matmul(g_y_weight, G_y)\n",
        "        G_bias = np.sum(G_y, axis=0)\n",
        "        G_input = np.matmul(G_y, g_y_input)\n",
        "        pm['w'] -= self.learning_rate * G_weight\n",
        "        pm['b'] -= self.learning_rate * G_bias\n",
        "        return G_input\n",
        "\n",
        "    def forward_postproc(self, output, y):\n",
        "        loss, aux_loss = self.dataset.forward_postproc(output, y)\n",
        "        extra, aux_extra = self.forward_extra_cost(y)\n",
        "        return loss + extra, [aux_loss, aux_extra]\n",
        "\n",
        "    def forward_extra_cost(self, y):\n",
        "        return 0, None\n",
        "\n",
        "    def backprop_postproc(self, G_loss, aux):\n",
        "        aux_loss, aux_extra = aux\n",
        "        self.backprop_extra_cost(G_loss, aux_extra)\n",
        "        G_output = self.dataset.backprop_postproc(G_loss, aux_loss)\n",
        "        return G_output\n",
        "\n",
        "    def backprop_extra_cost(self, G_loss, aux):\n",
        "        pass\n",
        "\n",
        "    def eval_accuracy(self, x, y, output=None):\n",
        "        if output is None:\n",
        "            output, _ = self.forward_neuralnet(x)\n",
        "        accuracy = self.dataset.eval_accuracy(x, y, output)\n",
        "        return accuracy\n",
        "\n",
        "    def get_estimate(self, x):\n",
        "        output, _ = self.forward_neuralnet(x)\n",
        "        estimate = self.dataset.get_estimate(output)\n",
        "        return estimate\n",
        "\n",
        "class AdamModel(MlpModel):\n",
        "    def __init__(self, name, dataset, hconfigs, dropout_p=None):\n",
        "        self.use_adam = True\n",
        "        super(AdamModel, self).__init__(name, dataset, hconfigs, dropout_p)\n",
        "\n",
        "    def backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "        if self.dropout_p is not None and self.is_training and hconfig is not None:\n",
        "            x, y, dropout_mask = aux\n",
        "            G_y *= dropout_mask\n",
        "        else:\n",
        "            x, y = aux\n",
        "        if hconfig is not None:\n",
        "            G_y = relu_derv(y) * G_y\n",
        "        g_y_weight = x.transpose()\n",
        "        g_y_input = pm['w'].transpose()\n",
        "        G_weight = np.matmul(g_y_weight, G_y)\n",
        "        G_bias = np.sum(G_y, axis=0)\n",
        "        G_input = np.matmul(G_y, g_y_input)\n",
        "        self.update_param(pm, 'w', G_weight)\n",
        "        self.update_param(pm, 'b', G_bias)\n",
        "        return G_input\n",
        "\n",
        "    def update_param(self, pm, key, delta):\n",
        "        if not self.use_adam:\n",
        "            pm[key] -= self.learning_rate * delta\n",
        "            return\n",
        "        ro_1, ro_2, eps = 0.9, 0.999, 1.0e-8\n",
        "        key_ro_1, key_ro_2 = 'm/' + key, 'v/' + key\n",
        "        if key_ro_1 not in pm:\n",
        "            pm[key_ro_1] = np.zeros(pm[key].shape)\n",
        "            pm[key_ro_2] = np.zeros(pm[key].shape)\n",
        "        pm[key_ro_1] = ro_1 * pm[key_ro_1] + (1 - ro_1) * delta\n",
        "        pm[key_ro_2] = ro_2 * pm[key_ro_2] + (1 - ro_2) * delta ** 2\n",
        "        m_hat = pm[key_ro_1] / (1 - ro_1)\n",
        "        v_hat = pm[key_ro_2] / (1 - ro_2)\n",
        "        pm[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + eps)"
      ],
      "metadata": {
        "id": "XxeqFJYGsOvs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. abalone dataset 불러오기"
      ],
      "metadata": {
        "id": "s88RLTA_OW-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "import copy    # chap 9\n",
        "import wave    # chap 11\n",
        "import cv2     # chap 12\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.core.display import HTML # chap 14\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def relu_derv(y):\n",
        "    return np.sign(y)\n",
        "def sigmoid(x):\n",
        "    return np.exp(-relu(-x)) / (1.0 + np.exp(-np.abs(x)))\n",
        "\n",
        "def sigmoid_derv(y):\n",
        "    return y * (1 - y)\n",
        "\n",
        "def sigmoid_cross_entropy_with_logits(z, x):\n",
        "    return relu(x) - x * z + np.log(1 + np.exp(-np.abs(x)))\n",
        "\n",
        "def sigmoid_cross_entropy_with_logits_derv(z, x):\n",
        "    return -z + sigmoid(x)\n",
        "def tanh(x):\n",
        "    return 2 * sigmoid(2*x) - 1\n",
        "\n",
        "def tanh_derv(y):\n",
        "    return (1.0 + y) * (1.0 - y)\n",
        "def softmax(x):\n",
        "    max_elem = np.max(x, axis=1)\n",
        "    diff = (x.transpose() - max_elem).transpose()\n",
        "    exp = np.exp(diff)\n",
        "    sum_exp = np.sum(exp, axis=1)\n",
        "    probs = (exp.transpose() / sum_exp).transpose()\n",
        "    return probs\n",
        "\n",
        "def softmax_cross_entropy_with_logits(labels, logits):\n",
        "    probs = softmax(logits)\n",
        "    return -np.sum(labels * np.log(probs+1.0e-10), axis=1)\n",
        "\n",
        "def softmax_cross_entropy_with_logits_derv(labels, logits):\n",
        "    return softmax(logits) - labels\n",
        "def load_csv(path, skip_header=True):\n",
        "    with open(path) as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        headers = None\n",
        "        if skip_header: headers = next(csvreader, None)\n",
        "        rows = []\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "    return rows, headers\n",
        "def onehot(xs, cnt):\n",
        "    return np.eye(cnt)[np.array(xs).astype(int)]\n",
        "\n",
        "def vector_to_str(x, fmt='%.2f', max_cnt=0):\n",
        "    if max_cnt == 0 or len(x) <= max_cnt:\n",
        "        return '[' + ','.join([fmt]*len(x)) % tuple(x) + ']'\n",
        "    v = x[0:max_cnt]\n",
        "    return '[' + ','.join([fmt]*len(v)) % tuple(v) + ',...]'\n",
        "def load_image_pixels(imagepath, resolution, input_shape):\n",
        "    img = Image.open(imagepath)\n",
        "    resized = img.resize(resolution)\n",
        "    return np.array(resized).reshape(input_shape)\n",
        "\n",
        "def draw_images_horz(xs, image_shape=None):\n",
        "    show_cnt = len(xs)\n",
        "    fig, axes = plt.subplots(1, show_cnt, figsize=(5,5))\n",
        "    for n in range(show_cnt):\n",
        "        img = xs[n]\n",
        "        if image_shape:\n",
        "            x3d = img.reshape(image_shape)\n",
        "            img = Image.fromarray(np.uint8(x3d))\n",
        "        axes[n].imshow(img)\n",
        "        axes[n].axis('off')\n",
        "    plt.draw()\n",
        "    plt.show()\n",
        "def show_select_results(est, ans, target_names, max_cnt=0):\n",
        "    for n in range(len(est)):\n",
        "        pstr = vector_to_str(100*est[n], '%2.0f', max_cnt)\n",
        "        estr = target_names[np.argmax(est[n])]\n",
        "        astr = target_names[np.argmax(ans[n])]\n",
        "        rstr = 'O'\n",
        "        if estr != astr: rstr = 'X'\n",
        "        print('추정확률분포 {} => 추정 {} : 정답 {} => {}'. \\\n",
        "              format(pstr, estr, astr, rstr))\n",
        "def list_dir(path):\n",
        "    filenames = os.listdir(path)\n",
        "    filenames.sort()\n",
        "    return filenames"
      ],
      "metadata": {
        "id": "-0fosyndOq4T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, name, mode):\n",
        "        self.name = name\n",
        "        self.mode = mode\n",
        "\n",
        "    def __str__(self):\n",
        "        return '{}({}, {}+{}+{})'.format(self.name, self.mode, \\\n",
        "                   len(self.tr_xs), len(self.te_xs), len(self.va_xs))\n",
        "\n",
        "    @property\n",
        "    def train_count(self):\n",
        "        return len(self.tr_xs)\n",
        "def dataset_get_train_data(self, batch_size, nth):\n",
        "    from_idx = nth * batch_size\n",
        "    to_idx = (nth + 1) * batch_size\n",
        "\n",
        "    tr_X = self.tr_xs[self.indices[from_idx:to_idx]]\n",
        "    tr_Y = self.tr_ys[self.indices[from_idx:to_idx]]\n",
        "\n",
        "    return tr_X, tr_Y\n",
        "\n",
        "\n",
        "def dataset_shuffle_train_data(self, size):\n",
        "    self.indices = np.arange(size)\n",
        "    np.random.shuffle(self.indices)\n",
        "\n",
        "Dataset.get_train_data = dataset_get_train_data\n",
        "Dataset.shuffle_train_data = dataset_shuffle_train_data\n",
        "def dataset_get_test_data(self):\n",
        "    return self.te_xs, self.te_ys\n",
        "\n",
        "Dataset.get_test_data = dataset_get_test_data\n",
        "def dataset_get_validate_data(self, count):\n",
        "    self.va_indices = np.arange(len(self.va_xs))\n",
        "    np.random.shuffle(self.va_indices)\n",
        "\n",
        "    va_X = self.va_xs[self.va_indices[0:count]]\n",
        "    va_Y = self.va_ys[self.va_indices[0:count]]\n",
        "\n",
        "    return va_X, va_Y\n",
        "\n",
        "Dataset.get_validate_data = dataset_get_validate_data\n",
        "Dataset.get_visualize_data = dataset_get_validate_data\n",
        "def dataset_shuffle_data(self, xs, ys, tr_ratio=0.8, va_ratio=0.05):\n",
        "    data_count = len(xs)\n",
        "\n",
        "    tr_cnt = int(data_count * tr_ratio / 10) * 10\n",
        "    va_cnt = int(data_count * va_ratio)\n",
        "    te_cnt = data_count - (tr_cnt + va_cnt)\n",
        "\n",
        "    tr_from, tr_to = 0, tr_cnt\n",
        "    va_from, va_to = tr_cnt, tr_cnt + va_cnt\n",
        "    te_from, te_to = tr_cnt + va_cnt, data_count\n",
        "\n",
        "    indices = np.arange(data_count)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    self.tr_xs = xs[indices[tr_from:tr_to]]\n",
        "    self.tr_ys = ys[indices[tr_from:tr_to]]\n",
        "    self.va_xs = xs[indices[va_from:va_to]]\n",
        "    self.va_ys = ys[indices[va_from:va_to]]\n",
        "    self.te_xs = xs[indices[te_from:te_to]]\n",
        "    self.te_ys = ys[indices[te_from:te_to]]\n",
        "\n",
        "    self.input_shape = xs[0].shape\n",
        "    self.output_shape = ys[0].shape\n",
        "\n",
        "    return indices[tr_from:tr_to], indices[va_from:va_to], indices[te_from:te_to]\n",
        "\n",
        "Dataset.shuffle_data = dataset_shuffle_data\n",
        "def dataset_forward_postproc(self, output, y, mode=None):\n",
        "    if mode is None: mode = self.mode\n",
        "\n",
        "    if mode == 'regression':\n",
        "        diff = output - y\n",
        "        square = np.square(diff)\n",
        "        loss = np.mean(square)\n",
        "        aux = diff\n",
        "    elif mode == 'binary':\n",
        "        entropy = sigmoid_cross_entropy_with_logits(y, output)\n",
        "        loss = np.mean(entropy)\n",
        "        aux = [y, output]\n",
        "    elif mode == 'select':\n",
        "        entropy = softmax_cross_entropy_with_logits(y, output)\n",
        "        loss = np.mean(entropy)\n",
        "        aux = [output, y, entropy]\n",
        "\n",
        "    return loss, aux\n",
        "\n",
        "Dataset.forward_postproc = dataset_forward_postproc\n",
        "def dataset_backprop_postproc(self, G_loss, aux, mode=None):\n",
        "    if mode is None: mode = self.mode\n",
        "\n",
        "    if mode == 'regression':\n",
        "        diff = aux\n",
        "        shape = diff.shape\n",
        "\n",
        "        g_loss_square = np.ones(shape) / np.prod(shape)\n",
        "        g_square_diff = 2 * diff\n",
        "        g_diff_output = 1\n",
        "\n",
        "        G_square = g_loss_square * G_loss\n",
        "        G_diff = g_square_diff * G_square\n",
        "        G_output = g_diff_output * G_diff\n",
        "    elif mode == 'binary':\n",
        "        y, output = aux\n",
        "        shape = output.shape\n",
        "\n",
        "        g_loss_entropy = np.ones(shape) / np.prod(shape)\n",
        "        g_entropy_output = sigmoid_cross_entropy_with_logits_derv(y, output)\n",
        "\n",
        "        G_entropy = g_loss_entropy * G_loss\n",
        "        G_output = g_entropy_output * G_entropy\n",
        "    elif mode == 'select':\n",
        "        output, y, entropy = aux\n",
        "\n",
        "        g_loss_entropy = 1.0 / np.prod(entropy.shape)\n",
        "        g_entropy_output = softmax_cross_entropy_with_logits_derv(y, output)\n",
        "\n",
        "        G_entropy = g_loss_entropy * G_loss\n",
        "        G_output = g_entropy_output * G_entropy\n",
        "\n",
        "    return G_output\n",
        "\n",
        "Dataset.backprop_postproc = dataset_backprop_postproc\n",
        "def dataset_eval_accuracy(self, x, y, output, mode=None):\n",
        "    if mode is None: mode = self.mode\n",
        "\n",
        "    if mode == 'regression':\n",
        "        mse = np.mean(np.square(output - y))\n",
        "        accuracy = 1 - np.sqrt(mse) / np.mean(y)\n",
        "    elif mode == 'binary':\n",
        "        estimate = np.greater(output, 0)\n",
        "        answer = np.equal(y, 1.0)\n",
        "        correct = np.equal(estimate, answer)\n",
        "        accuracy = np.mean(correct)\n",
        "    elif mode == 'select':\n",
        "        estimate = np.argmax(output, axis=1)\n",
        "        answer = np.argmax(y, axis=1)\n",
        "        correct = np.equal(estimate, answer)\n",
        "        accuracy = np.mean(correct)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "Dataset.eval_accuracy = dataset_eval_accuracy\n",
        "def dataset_get_estimate(self, output, mode=None):\n",
        "    if mode is None: mode = self.mode\n",
        "\n",
        "    if mode == 'regression':\n",
        "        estimate = output\n",
        "    elif mode == 'binary':\n",
        "        estimate = sigmoid(output)\n",
        "    elif mode == 'select':\n",
        "        estimate = softmax(output)\n",
        "\n",
        "    return estimate\n",
        "\n",
        "Dataset.get_estimate = dataset_get_estimate\n",
        "def dataset_train_prt_result(self, epoch, costs, accs, acc, time1, time2):\n",
        "    print('    Epoch {}: cost={:5.3f}, accuracy={:5.3f}/{:5.3f} ({}/{} secs)'. \\\n",
        "          format(epoch, np.mean(costs), np.mean(accs), acc, time1, time2))\n",
        "\n",
        "def dataset_test_prt_result(self, name, acc, time):\n",
        "    print('Model {} test report: accuracy = {:5.3f}, ({} secs)\\n'. \\\n",
        "          format(name, acc, time))\n",
        "\n",
        "Dataset.train_prt_result = dataset_train_prt_result\n",
        "Dataset.test_prt_result = dataset_test_prt_result"
      ],
      "metadata": {
        "id": "rAC5_YhZOggc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbaloneDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        super(AbaloneDataset, self).__init__('abalone', 'regression')\n",
        "\n",
        "        rows, _ = load_csv('/content/sample_data/abalone.csv')\n",
        "\n",
        "        xs = np.zeros([len(rows), 10])\n",
        "        ys = np.zeros([len(rows), 1])\n",
        "\n",
        "        for n, row in enumerate(rows):\n",
        "            if row[0] == 'I': xs[n, 0] = 1\n",
        "            if row[0] == 'M': xs[n, 1] = 1\n",
        "            if row[0] == 'F': xs[n, 2] = 1\n",
        "            xs[n, 3:] = row[1:-1]\n",
        "            ys[n, :] = row[-1:]\n",
        "\n",
        "        self.shuffle_data(xs, ys, 0.8)\n",
        "\n",
        "    def visualize(self, xs, estimates, answers):\n",
        "        for n in range(len(xs)):\n",
        "            x, est, ans = xs[n], estimates[n], answers[n]\n",
        "            xstr = vector_to_str(x, '%4.2f')\n",
        "            print('{} => 추정 {:4.1f} : 정답 {:4.1f}'.\n",
        "                  format(xstr, est[0], ans[0]))"
      ],
      "metadata": {
        "id": "pmIfLuPhN_nc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Adam 모델"
      ],
      "metadata": {
        "id": "TSoQzlurPoL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamModel(MlpModel):\n",
        "    def __init__(self, name, dataset, hconfigs):\n",
        "        self.use_adam = True\n",
        "        super(AdamModel, self).__init__(name, dataset, hconfigs)\n",
        "def adam_backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "    x, y = aux\n",
        "\n",
        "    if hconfig is not None: G_y = relu_derv(y) * G_y\n",
        "\n",
        "    g_y_weight = x.transpose()\n",
        "    g_y_input = pm['w'].transpose()\n",
        "\n",
        "    G_weight = np.matmul(g_y_weight, G_y)\n",
        "    G_bias = np.sum(G_y, axis=0)\n",
        "    G_input = np.matmul(G_y, g_y_input)\n",
        "\n",
        "    self.update_param(pm, 'w',  G_weight)\n",
        "    self.update_param(pm, 'b',  G_bias)\n",
        "\n",
        "    return G_input\n",
        "\n",
        "AdamModel.backprop_layer = adam_backprop_layer\n",
        "def adam_update_param(self, pm, key, delta):\n",
        "    if self.use_adam:\n",
        "        delta = self.eval_adam_delta(pm, key, delta)\n",
        "\n",
        "    pm[key] -= self.learning_rate * delta\n",
        "\n",
        "AdamModel.update_param = adam_update_param\n",
        "def adam_eval_adam_delta(self, pm, key, delta):\n",
        "    ro_1 = 0.9\n",
        "    ro_2 = 0.999\n",
        "    epsilon = 1.0e-8\n",
        "\n",
        "    skey, tkey, step = 's' + key, 't' + key, 'n' + key\n",
        "    if skey not in pm:\n",
        "        pm[skey] = np.zeros(pm[key].shape)\n",
        "        pm[tkey] = np.zeros(pm[key].shape)\n",
        "        pm[step] = 0\n",
        "\n",
        "    s = pm[skey] = ro_1 * pm[skey] + (1 - ro_1) * delta\n",
        "    t = pm[tkey] = ro_2 * pm[tkey] + (1 - ro_2) * (delta * delta)\n",
        "\n",
        "    pm[step] += 1\n",
        "    s = s / (1 - np.power(ro_1, pm[step]))\n",
        "    t = t / (1 - np.power(ro_2, pm[step]))\n",
        "\n",
        "    return s / (np.sqrt(t)+epsilon)\n",
        "\n",
        "AdamModel.eval_adam_delta = adam_eval_adam_delta"
      ],
      "metadata": {
        "id": "nvCMJBcsPrIV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 아다그라드 모델"
      ],
      "metadata": {
        "id": "92y73-RTWZn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# AdagradModel 클래스는 MlpModel 클래스를 상속받아 Adagrad 알고리즘을 사용하도록 확장\n",
        "class AdagradModel(MlpModel):\n",
        "    def __init__(self, name, dataset, hconfigs):\n",
        "        self.use_adagrad = True  # Adagrad 알고리즘 사용 여부를 설정\n",
        "        super(AdagradModel, self).__init__(name, dataset, hconfigs)  # 부모 클래스의 초기화 메서드를 호출\n",
        "\n",
        "# adagrad_backprop_layer 함수는 역전파 단계에서 각 층의 기울기를 계산하고 업데이트\n",
        "def adagrad_backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "    x, y = aux\n",
        "\n",
        "    if hconfig is not None:\n",
        "        G_y = relu_derv(y) * G_y  # 활성화 함수의 기울기를 곱하여 최종 기울기를 계산\n",
        "\n",
        "    g_y_weight = x.transpose()  # 입력 데이터의 전치 행렬\n",
        "    g_y_input = pm['w'].transpose()  # 가중치 행렬의 전치 행렬\n",
        "\n",
        "    G_weight = np.matmul(g_y_weight, G_y)  # 가중치에 대한 기울기 계산\n",
        "    G_bias = np.sum(G_y, axis=0)  # 편향에 대한 기울기 계산\n",
        "    G_input = np.matmul(G_y, g_y_input)  # 입력에 대한 기울기 계산\n",
        "\n",
        "    self.update_param(pm, 'w', G_weight)  # 가중치 파라미터 업데이트\n",
        "    self.update_param(pm, 'b', G_bias)  # 편향 파라미터 업데이트\n",
        "\n",
        "    return G_input  # 이전 층으로 전달할 기울기 반환\n",
        "\n",
        "# AdagradModel 클래스의 backprop_layer 메서드를 adagrad_backprop_layer 함수로 설정\n",
        "AdagradModel.backprop_layer = adagrad_backprop_layer\n",
        "\n",
        "# adagrad_update_param 함수는 파라미터 업데이트\n",
        "def adagrad_update_param(self, pm, key, delta):\n",
        "    if self.use_adagrad:\n",
        "        delta = self.eval_adagrad_delta(pm, key, delta)  # Adagrad 알고리즘을 적용하여 delta 값 조정\n",
        "\n",
        "    pm[key] -= self.learning_rate * delta  # 학습률을 곱한 delta 값을 사용하여 파라미터 업데이트\n",
        "\n",
        "# AdagradModel 클래스의 update_param 메서드를 adagrad_update_param 함수로 설정\n",
        "AdagradModel.update_param = adagrad_update_param\n",
        "\n",
        "# adagrad_eval_adagrad_delta 함수는 Adagrad 알고리즘을 적용하여 기울기(delta)를 조정\n",
        "def adagrad_eval_adagrad_delta(self, pm, key, delta):\n",
        "    epsilon = 1.0e-8  # 분모가 0이 되는 것을 방지하기 위한 작은 값\n",
        "\n",
        "    grad_squared_key = 'gs' + key  # 기울기 제곱합을 저장할 키 생성\n",
        "    if grad_squared_key not in pm:\n",
        "        pm[grad_squared_key] = np.zeros(pm[key].shape)  # 기울기 제곱합이 저장되지 않은 경우 0으로 초기화\n",
        "\n",
        "    pm[grad_squared_key] += delta * delta  # 기울기의 제곱을 기울기 제곱합에 누적\n",
        "\n",
        "    adjusted_delta = delta / (np.sqrt(pm[grad_squared_key]) + epsilon)  # 기울기 제곱합의 제곱근을 계산하여 delta 값을 조정\n",
        "    return adjusted_delta  # 조정된 delta 값 반환\n",
        "\n",
        "# AdagradModel 클래스의 eval_adagrad_delta 메서드를 adagrad_eval_adagrad_delta 함수로 설정\n",
        "AdagradModel.eval_adagrad_delta = adagrad_eval_adagrad_delta"
      ],
      "metadata": {
        "id": "KOL1qCjlW0SF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 알엠에스프롭"
      ],
      "metadata": {
        "id": "y2aSXCGbXEwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# RMSPropModel 클래스는 MlpModel 클래스를 상속받아 RMSProp 알고리즘을 사용하도록 확장\n",
        "class RMSPropModel(MlpModel):\n",
        "    def __init__(self, name, dataset, hconfigs):\n",
        "        self.use_rmsprop = True  # RMSProp 알고리즘 사용 여부를 설정\n",
        "        super(RMSPropModel, self).__init__(name, dataset, hconfigs)  # 부모 클래스의 초기화 메서드를 호출\n",
        "\n",
        "# rmsprop_backprop_layer 함수는 역전파 단계에서 각 층의 기울기를 계산하고 업데이트\n",
        "def rmsprop_backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "    x, y = aux\n",
        "\n",
        "    if hconfig is not None:\n",
        "        G_y = relu_derv(y) * G_y  # 활성화 함수의 기울기를 곱하여 최종 기울기를 계산\n",
        "\n",
        "    g_y_weight = x.transpose()  # 입력 데이터의 전치 행렬\n",
        "    g_y_input = pm['w'].transpose()  # 가중치 행렬의 전치 행렬\n",
        "\n",
        "    G_weight = np.matmul(g_y_weight, G_y)  # 가중치에 대한 기울기 계산\n",
        "    G_bias = np.sum(G_y, axis=0)  # 편향에 대한 기울기 계산\n",
        "    G_input = np.matmul(G_y, g_y_input)  # 입력에 대한 기울기 계산\n",
        "\n",
        "    self.update_param(pm, 'w', G_weight)  # 가중치 파라미터 업데이트\n",
        "    self.update_param(pm, 'b', G_bias)  # 편향 파라미터 업데이트\n",
        "\n",
        "    return G_input  # 이전 층으로 전달할 기울기 반환\n",
        "\n",
        "# RMSPropModel 클래스의 backprop_layer 메서드를 rmsprop_backprop_layer 함수로 설정\n",
        "RMSPropModel.backprop_layer = rmsprop_backprop_layer\n",
        "\n",
        "# rmsprop_update_param 함수는 파라미터 업데이트\n",
        "def rmsprop_update_param(self, pm, key, delta):\n",
        "    if self.use_rmsprop:\n",
        "        delta = self.eval_rmsprop_delta(pm, key, delta)  # RMSProp 알고리즘을 적용하여 delta 값 조정\n",
        "\n",
        "    pm[key] -= self.learning_rate * delta  # 학습률을 곱한 delta 값을 사용하여 파라미터 업데이트\n",
        "\n",
        "# RMSPropModel 클래스의 update_param 메서드를 rmsprop_update_param 함수로 설정\n",
        "RMSPropModel.update_param = rmsprop_update_param\n",
        "\n",
        "# rmsprop_eval_rmsprop_delta 함수는 RMSProp 알고리즘을 적용하여 기울기(delta)를 조정\n",
        "def rmsprop_eval_rmsprop_delta(self, pm, key, delta):\n",
        "    ro = 0.9  # 감쇠율\n",
        "    epsilon = 1.0e-8  # 분모가 0이 되는 것을 방지하기 위한 작은 값\n",
        "\n",
        "    mean_squared_key = 'ms' + key  # 기울기 제곱의 지수 가중 이동 평균을 저장할 키 생성\n",
        "    if mean_squared_key not in pm:\n",
        "        pm[mean_squared_key] = np.zeros(pm[key].shape)  # 기울기 제곱의 지수 가중 이동 평균이 저장되지 않은 경우 0으로 초기화\n",
        "\n",
        "    # 기울기 제곱의 지수 가중 이동 평균 업데이트\n",
        "    pm[mean_squared_key] = ro * pm[mean_squared_key] + (1 - ro) * (delta * delta)\n",
        "\n",
        "    # 기울기 제곱의 지수 가중 이동 평균의 제곱근을 계산하여 delta 값을 조정\n",
        "    adjusted_delta = delta / (np.sqrt(pm[mean_squared_key]) + epsilon)\n",
        "    return adjusted_delta  # 조정된 delta 값 반환\n",
        "\n",
        "# RMSPropModel 클래스의 eval_rmsprop_delta 메서드를 rmsprop_eval_rmsprop_delta 함수로 설정\n",
        "RMSPropModel.eval_rmsprop_delta = rmsprop_eval_rmsprop_delta"
      ],
      "metadata": {
        "id": "ZUQO3Vz7XJO9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 모멘텀"
      ],
      "metadata": {
        "id": "MLS3GW-pXztF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MomentumModel(MlpModel):\n",
        "    def __init__(self, name, dataset, hconfigs):\n",
        "        self.use_momentum = True\n",
        "        self.momentum_rate = 0.9  # 모멘텀 계수 설정\n",
        "        super(MomentumModel, self).__init__(name, dataset, hconfigs)\n",
        "\n",
        "def momentum_backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "    x, y = aux\n",
        "\n",
        "    if hconfig is not None:\n",
        "        G_y = relu_derv(y) * G_y\n",
        "\n",
        "    g_y_weight = x.transpose()\n",
        "    g_y_input = pm['w'].transpose()\n",
        "\n",
        "    G_weight = np.matmul(g_y_weight, G_y)\n",
        "    G_bias = np.sum(G_y, axis=0)\n",
        "    G_input = np.matmul(G_y, g_y_input)\n",
        "\n",
        "    self.update_param(pm, 'w', G_weight)\n",
        "    self.update_param(pm, 'b', G_bias)\n",
        "\n",
        "    return G_input\n",
        "\n",
        "MomentumModel.backprop_layer = momentum_backprop_layer\n",
        "\n",
        "def momentum_update_param(self, pm, key, delta):\n",
        "    if self.use_momentum:\n",
        "        delta = self.eval_momentum_delta(pm, key, delta)\n",
        "\n",
        "    pm[key] -= self.learning_rate * delta\n",
        "\n",
        "MomentumModel.update_param = momentum_update_param\n",
        "\n",
        "def momentum_eval_momentum_delta(self, pm, key, delta):\n",
        "    vkey = 'v' + key\n",
        "\n",
        "    if vkey not in pm:\n",
        "        pm[vkey] = np.zeros(pm[key].shape)\n",
        "\n",
        "    pm[vkey] = self.momentum_rate * pm[vkey] + delta\n",
        "\n",
        "    return pm[vkey]\n",
        "\n",
        "MomentumModel.eval_momentum_delta = momentum_eval_momentum_delta\n"
      ],
      "metadata": {
        "id": "KmuVKLc2X4RV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 네스테로프 모멘텀"
      ],
      "metadata": {
        "id": "7KAH9cW7X7Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NesterovMomentumModel(MlpModel):\n",
        "    def __init__(self, name, dataset, hconfigs):\n",
        "        self.use_nesterov_momentum = True\n",
        "        self.momentum_rate = 0.9  # 모멘텀 계수 설정\n",
        "        super(NesterovMomentumModel, self).__init__(name, dataset, hconfigs)\n",
        "\n",
        "def nesterov_backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "    x, y = aux\n",
        "\n",
        "    if hconfig is not None:\n",
        "        G_y = relu_derv(y) * G_y\n",
        "\n",
        "    g_y_weight = x.transpose()\n",
        "    g_y_input = pm['w'].transpose()\n",
        "\n",
        "    G_weight = np.matmul(g_y_weight, G_y)\n",
        "    G_bias = np.sum(G_y, axis=0)\n",
        "    G_input = np.matmul(G_y, g_y_input)\n",
        "\n",
        "    self.update_param(pm, 'w', G_weight)\n",
        "    self.update_param(pm, 'b', G_bias)\n",
        "\n",
        "    return G_input\n",
        "\n",
        "NesterovMomentumModel.backprop_layer = nesterov_backprop_layer\n",
        "\n",
        "def nesterov_update_param(self, pm, key, delta):\n",
        "    if self.use_nesterov_momentum:\n",
        "        delta = self.eval_nesterov_momentum_delta(pm, key, delta)\n",
        "\n",
        "    pm[key] -= self.learning_rate * delta\n",
        "\n",
        "NesterovMomentumModel.update_param = nesterov_update_param\n",
        "\n",
        "def nesterov_eval_nesterov_momentum_delta(self, pm, key, delta):\n",
        "    vkey = 'v' + key\n",
        "\n",
        "    if vkey not in pm:\n",
        "        pm[vkey] = np.zeros(pm[key].shape)\n",
        "\n",
        "    v_prev = pm[vkey]\n",
        "    pm[vkey] = self.momentum_rate * pm[vkey] + delta\n",
        "    nesterov_delta = self.momentum_rate * pm[vkey] + (1 + self.momentum_rate) * delta\n",
        "\n",
        "    return nesterov_delta\n",
        "\n",
        "NesterovMomentumModel.eval_nesterov_momentum_delta = nesterov_eval_nesterov_momentum_delta\n"
      ],
      "metadata": {
        "id": "931lkq1AX9xV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 아다델타"
      ],
      "metadata": {
        "id": "iAWgGUhDX-Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class AdadeltaModel(MlpModel):\n",
        "    def __init__(self, name, dataset, hconfigs):\n",
        "        self.use_adadelta = True\n",
        "        super(AdadeltaModel, self).__init__(name, dataset, hconfigs)\n",
        "\n",
        "def adadelta_backprop_layer(self, G_y, hconfig, pm, aux):\n",
        "    x, y = aux\n",
        "\n",
        "    if hconfig is not None:\n",
        "        G_y = relu_derv(y) * G_y\n",
        "\n",
        "    g_y_weight = x.transpose()\n",
        "    g_y_input = pm['w'].transpose()\n",
        "\n",
        "    G_weight = np.matmul(g_y_weight, G_y)\n",
        "    G_bias = np.sum(G_y, axis=0)\n",
        "    G_input = np.matmul(G_y, g_y_input)\n",
        "\n",
        "    self.update_param(pm, 'w', G_weight)\n",
        "    self.update_param(pm, 'b', G_bias)\n",
        "\n",
        "    return G_input\n",
        "\n",
        "AdadeltaModel.backprop_layer = adadelta_backprop_layer\n",
        "\n",
        "def adadelta_update_param(self, pm, key, delta):\n",
        "    if self.use_adadelta:\n",
        "        delta = self.eval_adadelta_delta(pm, key, delta)\n",
        "\n",
        "    pm[key] -= delta\n",
        "\n",
        "AdadeltaModel.update_param = adadelta_update_param\n",
        "\n",
        "def adadelta_eval_adadelta_delta(self, pm, key, delta):\n",
        "    ro = 0.95\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    skey, tkey, step = 's' + key, 't' + key, 'n' + key\n",
        "\n",
        "    if skey not in pm:\n",
        "        pm[skey] = np.zeros(pm[key].shape)\n",
        "        pm[tkey] = np.zeros(pm[key].shape)\n",
        "\n",
        "    pm[skey] = ro * pm[skey] + (1 - ro) * np.square(delta)\n",
        "    update = np.sqrt((pm[tkey] + epsilon) / (pm[skey] + epsilon)) * delta\n",
        "    pm[tkey] = ro * pm[tkey] + (1 - ro) * np.square(update)\n",
        "\n",
        "    return update\n",
        "\n",
        "AdadeltaModel.eval_adadelta_delta = adadelta_eval_adadelta_delta\n"
      ],
      "metadata": {
        "id": "fU3NLVCRYDbF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. MLP 실행"
      ],
      "metadata": {
        "id": "cSATtlYXO6ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) Adam 모델"
      ],
      "metadata": {
        "id": "nOZwPxrQfTsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ad = AbaloneDataset()\n",
        "am = AdamModel('abalone_model', ad, [200])\n",
        "am.exec_all(epoch_count=150, report=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeKde4vRPZOc",
        "outputId": "8337a31f-03c3-4a0b-d73e-67a0c19b587b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model abalone_model train started:\n",
            "    Epoch 2: cost=5.841, accuracy=0.770/0.789 (1/1 secs)\n",
            "    Epoch 4: cost=5.243, accuracy=0.783/0.790 (0/1 secs)\n",
            "    Epoch 6: cost=5.157, accuracy=0.784/0.794 (0/1 secs)\n",
            "    Epoch 8: cost=4.951, accuracy=0.789/0.788 (0/1 secs)\n",
            "    Epoch 10: cost=5.004, accuracy=0.789/0.748 (1/2 secs)\n",
            "    Epoch 12: cost=5.146, accuracy=0.785/0.799 (0/2 secs)\n",
            "    Epoch 14: cost=4.835, accuracy=0.792/0.795 (0/2 secs)\n",
            "    Epoch 16: cost=4.910, accuracy=0.791/0.789 (0/2 secs)\n",
            "    Epoch 18: cost=4.830, accuracy=0.792/0.790 (1/3 secs)\n",
            "    Epoch 20: cost=4.754, accuracy=0.793/0.779 (0/3 secs)\n",
            "    Epoch 22: cost=4.664, accuracy=0.796/0.770 (0/3 secs)\n",
            "    Epoch 24: cost=4.686, accuracy=0.795/0.790 (1/4 secs)\n",
            "    Epoch 26: cost=4.721, accuracy=0.794/0.817 (0/4 secs)\n",
            "    Epoch 28: cost=4.641, accuracy=0.796/0.770 (0/4 secs)\n",
            "    Epoch 30: cost=4.686, accuracy=0.795/0.789 (1/5 secs)\n",
            "    Epoch 32: cost=4.659, accuracy=0.796/0.798 (0/5 secs)\n",
            "    Epoch 34: cost=4.634, accuracy=0.797/0.778 (0/5 secs)\n",
            "    Epoch 36: cost=4.537, accuracy=0.799/0.805 (1/6 secs)\n",
            "    Epoch 38: cost=4.576, accuracy=0.796/0.790 (0/6 secs)\n",
            "    Epoch 40: cost=4.507, accuracy=0.800/0.768 (0/6 secs)\n",
            "    Epoch 42: cost=4.529, accuracy=0.799/0.779 (1/7 secs)\n",
            "    Epoch 44: cost=4.596, accuracy=0.797/0.796 (0/7 secs)\n",
            "    Epoch 46: cost=4.512, accuracy=0.797/0.770 (0/7 secs)\n",
            "    Epoch 48: cost=4.556, accuracy=0.799/0.811 (1/8 secs)\n",
            "    Epoch 50: cost=4.522, accuracy=0.799/0.793 (0/8 secs)\n",
            "    Epoch 52: cost=4.557, accuracy=0.798/0.792 (0/8 secs)\n",
            "    Epoch 54: cost=4.576, accuracy=0.797/0.776 (1/9 secs)\n",
            "    Epoch 56: cost=4.545, accuracy=0.799/0.790 (0/9 secs)\n",
            "    Epoch 58: cost=4.535, accuracy=0.800/0.792 (0/9 secs)\n",
            "    Epoch 60: cost=4.604, accuracy=0.799/0.783 (1/10 secs)\n",
            "    Epoch 62: cost=4.520, accuracy=0.798/0.779 (0/10 secs)\n",
            "    Epoch 64: cost=4.537, accuracy=0.799/0.795 (0/10 secs)\n",
            "    Epoch 66: cost=4.524, accuracy=0.799/0.783 (1/11 secs)\n",
            "    Epoch 68: cost=4.513, accuracy=0.800/0.777 (0/11 secs)\n",
            "    Epoch 70: cost=4.462, accuracy=0.801/0.770 (0/11 secs)\n",
            "    Epoch 72: cost=4.491, accuracy=0.799/0.806 (1/12 secs)\n",
            "    Epoch 74: cost=4.481, accuracy=0.799/0.808 (1/13 secs)\n",
            "    Epoch 76: cost=4.437, accuracy=0.800/0.791 (0/13 secs)\n",
            "    Epoch 78: cost=4.517, accuracy=0.799/0.786 (0/13 secs)\n",
            "    Epoch 80: cost=4.438, accuracy=0.800/0.781 (1/14 secs)\n",
            "    Epoch 82: cost=4.426, accuracy=0.802/0.788 (0/14 secs)\n",
            "    Epoch 84: cost=4.471, accuracy=0.800/0.800 (0/14 secs)\n",
            "    Epoch 86: cost=4.430, accuracy=0.802/0.794 (1/15 secs)\n",
            "    Epoch 88: cost=4.373, accuracy=0.802/0.790 (0/15 secs)\n",
            "    Epoch 90: cost=4.407, accuracy=0.803/0.792 (1/16 secs)\n",
            "    Epoch 92: cost=4.405, accuracy=0.801/0.798 (0/16 secs)\n",
            "    Epoch 94: cost=4.424, accuracy=0.802/0.789 (0/16 secs)\n",
            "    Epoch 96: cost=4.377, accuracy=0.802/0.795 (1/17 secs)\n",
            "    Epoch 98: cost=4.443, accuracy=0.802/0.794 (0/17 secs)\n",
            "    Epoch 100: cost=4.379, accuracy=0.802/0.780 (0/17 secs)\n",
            "    Epoch 102: cost=4.411, accuracy=0.799/0.792 (1/18 secs)\n",
            "    Epoch 104: cost=4.398, accuracy=0.801/0.779 (0/18 secs)\n",
            "    Epoch 106: cost=4.369, accuracy=0.802/0.784 (0/18 secs)\n",
            "    Epoch 108: cost=4.334, accuracy=0.803/0.770 (1/19 secs)\n",
            "    Epoch 110: cost=4.449, accuracy=0.801/0.800 (0/19 secs)\n",
            "    Epoch 112: cost=4.417, accuracy=0.801/0.787 (0/19 secs)\n",
            "    Epoch 114: cost=4.461, accuracy=0.799/0.789 (1/20 secs)\n",
            "    Epoch 116: cost=4.368, accuracy=0.803/0.786 (0/20 secs)\n",
            "    Epoch 118: cost=4.348, accuracy=0.804/0.804 (0/20 secs)\n",
            "    Epoch 120: cost=4.378, accuracy=0.802/0.795 (1/21 secs)\n",
            "    Epoch 122: cost=4.346, accuracy=0.803/0.820 (0/21 secs)\n",
            "    Epoch 124: cost=4.371, accuracy=0.803/0.797 (0/21 secs)\n",
            "    Epoch 126: cost=4.370, accuracy=0.802/0.804 (1/22 secs)\n",
            "    Epoch 128: cost=4.415, accuracy=0.800/0.808 (0/22 secs)\n",
            "    Epoch 130: cost=4.325, accuracy=0.802/0.812 (0/22 secs)\n",
            "    Epoch 132: cost=4.370, accuracy=0.804/0.794 (1/23 secs)\n",
            "    Epoch 134: cost=4.363, accuracy=0.803/0.787 (0/23 secs)\n",
            "    Epoch 136: cost=4.381, accuracy=0.802/0.802 (0/23 secs)\n",
            "    Epoch 138: cost=4.342, accuracy=0.804/0.808 (1/24 secs)\n",
            "    Epoch 140: cost=4.387, accuracy=0.803/0.793 (0/24 secs)\n",
            "    Epoch 142: cost=4.308, accuracy=0.805/0.818 (1/25 secs)\n",
            "    Epoch 144: cost=4.334, accuracy=0.803/0.772 (0/25 secs)\n",
            "    Epoch 146: cost=4.358, accuracy=0.802/0.805 (1/26 secs)\n",
            "    Epoch 148: cost=4.409, accuracy=0.800/0.799 (0/26 secs)\n",
            "    Epoch 150: cost=4.422, accuracy=0.802/0.793 (1/27 secs)\n",
            "Model abalone_model train ended in 27 secs:\n",
            "Model abalone_model test report: accuracy = 0.792, (0 secs)\n",
            "\n",
            "Model abalone_model Visualization\n",
            "[0.00,1.00,0.00,0.57,0.46,0.17,0.90,0.41,0.19,0.21] => 추정  9.5 : 정답  7.0\n",
            "[0.00,0.00,1.00,0.57,0.44,0.12,0.86,0.37,0.17,0.27] => 추정 11.1 : 정답 12.0\n",
            "[0.00,1.00,0.00,0.58,0.45,0.14,0.82,0.35,0.18,0.26] => 추정 10.9 : 정답 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) Adadelta 모델"
      ],
      "metadata": {
        "id": "sP5_aH1JfZVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ad = AbaloneDataset()\n",
        "am = AdadeltaModel('AdadeltaModel', ad, [200])\n",
        "am.exec_all(epoch_count=150, report=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VcZ3AqLY7wd",
        "outputId": "7d423508-717e-4cab-f754-3be17577be96"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model AdadeltaModel train started:\n",
            "    Epoch 2: cost=6.060, accuracy=0.769/0.726 (1/1 secs)\n",
            "    Epoch 4: cost=5.305, accuracy=0.784/0.811 (0/1 secs)\n",
            "    Epoch 6: cost=5.148, accuracy=0.787/0.779 (0/1 secs)\n",
            "    Epoch 8: cost=4.996, accuracy=0.791/0.835 (0/1 secs)\n",
            "    Epoch 10: cost=4.991, accuracy=0.790/0.817 (0/1 secs)\n",
            "    Epoch 12: cost=4.954, accuracy=0.790/0.764 (1/2 secs)\n",
            "    Epoch 14: cost=4.854, accuracy=0.792/0.774 (0/2 secs)\n",
            "    Epoch 16: cost=4.944, accuracy=0.792/0.793 (0/2 secs)\n",
            "    Epoch 18: cost=4.840, accuracy=0.793/0.803 (1/3 secs)\n",
            "    Epoch 20: cost=4.823, accuracy=0.794/0.795 (0/3 secs)\n",
            "    Epoch 22: cost=4.841, accuracy=0.794/0.751 (0/3 secs)\n",
            "    Epoch 24: cost=4.809, accuracy=0.795/0.821 (1/4 secs)\n",
            "    Epoch 26: cost=4.786, accuracy=0.795/0.784 (0/4 secs)\n",
            "    Epoch 28: cost=4.752, accuracy=0.794/0.763 (0/4 secs)\n",
            "    Epoch 30: cost=4.787, accuracy=0.795/0.772 (1/5 secs)\n",
            "    Epoch 32: cost=4.754, accuracy=0.796/0.802 (0/5 secs)\n",
            "    Epoch 34: cost=4.738, accuracy=0.797/0.791 (0/5 secs)\n",
            "    Epoch 36: cost=4.732, accuracy=0.796/0.797 (0/5 secs)\n",
            "    Epoch 38: cost=4.701, accuracy=0.796/0.819 (0/5 secs)\n",
            "    Epoch 40: cost=4.707, accuracy=0.798/0.797 (1/6 secs)\n",
            "    Epoch 42: cost=4.682, accuracy=0.797/0.786 (0/6 secs)\n",
            "    Epoch 44: cost=4.608, accuracy=0.798/0.783 (0/6 secs)\n",
            "    Epoch 46: cost=4.656, accuracy=0.798/0.802 (0/6 secs)\n",
            "    Epoch 48: cost=4.678, accuracy=0.796/0.780 (1/7 secs)\n",
            "    Epoch 50: cost=4.644, accuracy=0.799/0.811 (0/7 secs)\n",
            "    Epoch 52: cost=4.683, accuracy=0.797/0.795 (0/7 secs)\n",
            "    Epoch 54: cost=4.637, accuracy=0.797/0.778 (0/7 secs)\n",
            "    Epoch 56: cost=4.635, accuracy=0.799/0.793 (1/8 secs)\n",
            "    Epoch 58: cost=4.592, accuracy=0.799/0.797 (0/8 secs)\n",
            "    Epoch 60: cost=4.632, accuracy=0.798/0.778 (0/8 secs)\n",
            "    Epoch 62: cost=4.563, accuracy=0.799/0.788 (0/8 secs)\n",
            "    Epoch 64: cost=4.579, accuracy=0.800/0.869 (1/9 secs)\n",
            "    Epoch 66: cost=4.609, accuracy=0.799/0.794 (0/9 secs)\n",
            "    Epoch 68: cost=4.627, accuracy=0.797/0.797 (0/9 secs)\n",
            "    Epoch 70: cost=4.597, accuracy=0.798/0.818 (0/9 secs)\n",
            "    Epoch 72: cost=4.563, accuracy=0.801/0.830 (1/10 secs)\n",
            "    Epoch 74: cost=4.541, accuracy=0.800/0.770 (0/10 secs)\n",
            "    Epoch 76: cost=4.557, accuracy=0.799/0.769 (0/10 secs)\n",
            "    Epoch 78: cost=4.577, accuracy=0.799/0.779 (0/10 secs)\n",
            "    Epoch 80: cost=4.574, accuracy=0.800/0.789 (1/11 secs)\n",
            "    Epoch 82: cost=4.575, accuracy=0.799/0.826 (0/11 secs)\n",
            "    Epoch 84: cost=4.535, accuracy=0.801/0.808 (0/11 secs)\n",
            "    Epoch 86: cost=4.560, accuracy=0.799/0.773 (0/11 secs)\n",
            "    Epoch 88: cost=4.547, accuracy=0.799/0.770 (1/12 secs)\n",
            "    Epoch 90: cost=4.564, accuracy=0.799/0.801 (0/12 secs)\n",
            "    Epoch 92: cost=4.516, accuracy=0.801/0.803 (0/12 secs)\n",
            "    Epoch 94: cost=4.570, accuracy=0.800/0.817 (0/12 secs)\n",
            "    Epoch 96: cost=4.530, accuracy=0.802/0.805 (1/13 secs)\n",
            "    Epoch 98: cost=4.499, accuracy=0.800/0.831 (0/13 secs)\n",
            "    Epoch 100: cost=4.513, accuracy=0.799/0.778 (0/13 secs)\n",
            "    Epoch 102: cost=4.518, accuracy=0.801/0.755 (0/13 secs)\n",
            "    Epoch 104: cost=4.496, accuracy=0.800/0.817 (1/14 secs)\n",
            "    Epoch 106: cost=4.515, accuracy=0.799/0.774 (0/14 secs)\n",
            "    Epoch 108: cost=4.526, accuracy=0.800/0.824 (0/14 secs)\n",
            "    Epoch 110: cost=4.482, accuracy=0.801/0.782 (0/14 secs)\n",
            "    Epoch 112: cost=4.484, accuracy=0.802/0.763 (1/15 secs)\n",
            "    Epoch 114: cost=4.494, accuracy=0.800/0.811 (0/15 secs)\n",
            "    Epoch 116: cost=4.502, accuracy=0.800/0.769 (1/16 secs)\n",
            "    Epoch 118: cost=4.500, accuracy=0.801/0.791 (0/16 secs)\n",
            "    Epoch 120: cost=4.508, accuracy=0.801/0.791 (0/16 secs)\n",
            "    Epoch 122: cost=4.467, accuracy=0.801/0.816 (1/17 secs)\n",
            "    Epoch 124: cost=4.473, accuracy=0.801/0.780 (0/17 secs)\n",
            "    Epoch 126: cost=4.490, accuracy=0.801/0.766 (0/17 secs)\n",
            "    Epoch 128: cost=4.471, accuracy=0.801/0.810 (1/18 secs)\n",
            "    Epoch 130: cost=4.448, accuracy=0.802/0.773 (0/18 secs)\n",
            "    Epoch 132: cost=4.522, accuracy=0.800/0.771 (0/18 secs)\n",
            "    Epoch 134: cost=4.495, accuracy=0.802/0.790 (0/18 secs)\n",
            "    Epoch 136: cost=4.483, accuracy=0.800/0.789 (1/19 secs)\n",
            "    Epoch 138: cost=4.491, accuracy=0.801/0.818 (0/19 secs)\n",
            "    Epoch 140: cost=4.464, accuracy=0.801/0.798 (0/19 secs)\n",
            "    Epoch 142: cost=4.462, accuracy=0.802/0.782 (0/19 secs)\n",
            "    Epoch 144: cost=4.473, accuracy=0.800/0.819 (1/20 secs)\n",
            "    Epoch 146: cost=4.458, accuracy=0.803/0.800 (0/20 secs)\n",
            "    Epoch 148: cost=4.448, accuracy=0.802/0.816 (0/20 secs)\n",
            "    Epoch 150: cost=4.454, accuracy=0.803/0.794 (0/20 secs)\n",
            "Model AdadeltaModel train ended in 20 secs:\n",
            "Model AdadeltaModel test report: accuracy = 0.805, (0 secs)\n",
            "\n",
            "Model AdadeltaModel Visualization\n",
            "[0.00,1.00,0.00,0.60,0.47,0.13,1.01,0.42,0.22,0.30] => 추정 10.4 : 정답  9.0\n",
            "[0.00,0.00,1.00,0.58,0.47,0.14,0.91,0.38,0.16,0.32] => 추정 11.7 : 정답 13.0\n",
            "[0.00,1.00,0.00,0.49,0.40,0.12,0.66,0.26,0.16,0.19] => 추정  9.7 : 정답 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) NestroMomentumModel"
      ],
      "metadata": {
        "id": "uNqEVN74feDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ad = AbaloneDataset()\n",
        "am = NesterovMomentumModel('NesterovMomentumModel', ad, [500])\n",
        "am.exec_all(epoch_count=150, report=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry2qH1IZan3g",
        "outputId": "fe8340e7-6251-483b-abe5-17cd63eb7529"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model NesterovMomentumModel train started:\n",
            "    Epoch 2: cost=5.621, accuracy=0.778/0.802 (0/0 secs)\n",
            "    Epoch 4: cost=5.471, accuracy=0.781/0.774 (0/0 secs)\n",
            "    Epoch 6: cost=5.578, accuracy=0.778/0.733 (0/0 secs)\n",
            "    Epoch 8: cost=5.364, accuracy=0.783/0.764 (1/1 secs)\n",
            "    Epoch 10: cost=5.197, accuracy=0.788/0.776 (0/1 secs)\n",
            "    Epoch 12: cost=5.110, accuracy=0.788/0.807 (0/1 secs)\n",
            "    Epoch 14: cost=5.025, accuracy=0.789/0.818 (0/1 secs)\n",
            "    Epoch 16: cost=5.049, accuracy=0.789/0.764 (1/2 secs)\n",
            "    Epoch 18: cost=4.919, accuracy=0.792/0.805 (0/2 secs)\n",
            "    Epoch 20: cost=4.908, accuracy=0.791/0.770 (0/2 secs)\n",
            "    Epoch 22: cost=5.020, accuracy=0.788/0.799 (1/3 secs)\n",
            "    Epoch 24: cost=4.889, accuracy=0.792/0.779 (0/3 secs)\n",
            "    Epoch 26: cost=4.840, accuracy=0.794/0.775 (0/3 secs)\n",
            "    Epoch 28: cost=4.935, accuracy=0.791/0.826 (1/4 secs)\n",
            "    Epoch 30: cost=4.830, accuracy=0.794/0.788 (0/4 secs)\n",
            "    Epoch 32: cost=4.888, accuracy=0.794/0.754 (0/4 secs)\n",
            "    Epoch 34: cost=5.085, accuracy=0.786/0.814 (1/5 secs)\n",
            "    Epoch 36: cost=4.851, accuracy=0.792/0.794 (0/5 secs)\n",
            "    Epoch 38: cost=4.787, accuracy=0.793/0.761 (1/6 secs)\n",
            "    Epoch 40: cost=4.856, accuracy=0.794/0.785 (0/6 secs)\n",
            "    Epoch 42: cost=4.774, accuracy=0.794/0.789 (0/6 secs)\n",
            "    Epoch 44: cost=4.845, accuracy=0.792/0.798 (1/7 secs)\n",
            "    Epoch 46: cost=4.889, accuracy=0.791/0.799 (0/7 secs)\n",
            "    Epoch 48: cost=4.860, accuracy=0.793/0.782 (1/8 secs)\n",
            "    Epoch 50: cost=4.900, accuracy=0.791/0.787 (1/9 secs)\n",
            "    Epoch 52: cost=4.779, accuracy=0.796/0.807 (0/9 secs)\n",
            "    Epoch 54: cost=4.734, accuracy=0.794/0.770 (1/10 secs)\n",
            "    Epoch 56: cost=4.747, accuracy=0.794/0.747 (0/10 secs)\n",
            "    Epoch 58: cost=4.972, accuracy=0.789/0.786 (0/10 secs)\n",
            "    Epoch 60: cost=5.050, accuracy=0.786/0.785 (1/11 secs)\n",
            "    Epoch 62: cost=4.752, accuracy=0.794/0.757 (0/11 secs)\n",
            "    Epoch 64: cost=4.737, accuracy=0.795/0.783 (1/12 secs)\n",
            "    Epoch 66: cost=4.744, accuracy=0.795/0.791 (0/12 secs)\n",
            "    Epoch 68: cost=4.804, accuracy=0.793/0.774 (0/12 secs)\n",
            "    Epoch 70: cost=4.804, accuracy=0.794/0.769 (1/13 secs)\n",
            "    Epoch 72: cost=4.820, accuracy=0.792/0.776 (0/13 secs)\n",
            "    Epoch 74: cost=4.798, accuracy=0.793/0.815 (1/14 secs)\n",
            "    Epoch 76: cost=4.821, accuracy=0.793/0.790 (0/14 secs)\n",
            "    Epoch 78: cost=4.777, accuracy=0.795/0.767 (0/14 secs)\n",
            "    Epoch 80: cost=4.796, accuracy=0.793/0.787 (1/15 secs)\n",
            "    Epoch 82: cost=4.913, accuracy=0.792/0.791 (0/15 secs)\n",
            "    Epoch 84: cost=4.756, accuracy=0.795/0.756 (0/15 secs)\n",
            "    Epoch 86: cost=4.755, accuracy=0.796/0.770 (1/16 secs)\n",
            "    Epoch 88: cost=4.629, accuracy=0.797/0.758 (0/16 secs)\n",
            "    Epoch 90: cost=4.837, accuracy=0.792/0.813 (1/17 secs)\n",
            "    Epoch 92: cost=4.714, accuracy=0.795/0.778 (0/17 secs)\n",
            "    Epoch 94: cost=4.820, accuracy=0.794/0.781 (0/17 secs)\n",
            "    Epoch 96: cost=4.764, accuracy=0.793/0.755 (1/18 secs)\n",
            "    Epoch 98: cost=4.732, accuracy=0.796/0.782 (0/18 secs)\n",
            "    Epoch 100: cost=4.921, accuracy=0.790/0.790 (1/19 secs)\n",
            "    Epoch 102: cost=4.860, accuracy=0.791/0.795 (0/19 secs)\n",
            "    Epoch 104: cost=4.785, accuracy=0.794/0.771 (0/19 secs)\n",
            "    Epoch 106: cost=4.814, accuracy=0.793/0.807 (1/20 secs)\n",
            "    Epoch 108: cost=4.908, accuracy=0.790/0.763 (1/21 secs)\n",
            "    Epoch 110: cost=4.869, accuracy=0.791/0.778 (0/21 secs)\n",
            "    Epoch 112: cost=4.867, accuracy=0.790/0.785 (1/22 secs)\n",
            "    Epoch 114: cost=4.879, accuracy=0.789/0.791 (0/22 secs)\n",
            "    Epoch 116: cost=4.823, accuracy=0.794/0.787 (1/23 secs)\n",
            "    Epoch 118: cost=4.790, accuracy=0.793/0.776 (0/23 secs)\n",
            "    Epoch 120: cost=4.875, accuracy=0.792/0.785 (0/23 secs)\n",
            "    Epoch 122: cost=4.790, accuracy=0.793/0.801 (1/24 secs)\n",
            "    Epoch 124: cost=4.739, accuracy=0.794/0.793 (0/24 secs)\n",
            "    Epoch 126: cost=4.824, accuracy=0.794/0.789 (1/25 secs)\n",
            "    Epoch 128: cost=4.801, accuracy=0.794/0.790 (0/25 secs)\n",
            "    Epoch 130: cost=4.757, accuracy=0.792/0.759 (0/25 secs)\n",
            "    Epoch 132: cost=4.775, accuracy=0.794/0.787 (1/26 secs)\n",
            "    Epoch 134: cost=4.675, accuracy=0.795/0.791 (0/26 secs)\n",
            "    Epoch 136: cost=4.660, accuracy=0.796/0.808 (0/26 secs)\n",
            "    Epoch 138: cost=4.814, accuracy=0.795/0.750 (1/27 secs)\n",
            "    Epoch 140: cost=4.673, accuracy=0.797/0.779 (0/27 secs)\n",
            "    Epoch 142: cost=4.760, accuracy=0.793/0.795 (1/28 secs)\n",
            "    Epoch 144: cost=4.829, accuracy=0.792/0.729 (0/28 secs)\n",
            "    Epoch 146: cost=4.758, accuracy=0.792/0.800 (0/28 secs)\n",
            "    Epoch 148: cost=4.801, accuracy=0.792/0.791 (1/29 secs)\n",
            "    Epoch 150: cost=4.802, accuracy=0.792/0.814 (0/29 secs)\n",
            "Model NesterovMomentumModel train ended in 29 secs:\n",
            "Model NesterovMomentumModel test report: accuracy = 0.780, (0 secs)\n",
            "\n",
            "Model NesterovMomentumModel Visualization\n",
            "[0.00,0.00,1.00,0.66,0.54,0.21,1.56,0.69,0.30,0.44] => 추정 12.2 : 정답 11.0\n",
            "[1.00,0.00,0.00,0.42,0.33,0.11,0.33,0.12,0.08,0.10] => 추정  7.9 : 정답  7.0\n",
            "[0.00,1.00,0.00,0.52,0.43,0.20,0.97,0.30,0.21,0.35] => 추정 15.4 : 정답 18.0\n"
          ]
        }
      ]
    }
  ]
}